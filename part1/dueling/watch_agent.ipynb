{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DUELING DQN AGENT VISUALISATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook containing the code to generate videos of the agent playing the game once trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import wandb\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn        \n",
    "import torch.optim as optim \n",
    "from torchsummary import summary\n",
    "import collections\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from PIL import Image\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm\n",
    "import ale_py\n",
    "from gymnasium.wrappers import MaxAndSkipObservation, ResizeObservation, GrayscaleObservation, FrameStackObservation, ReshapeObservation\n",
    "from collections import namedtuple, deque\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+6a7e0ae)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "gym.register_envs(ale_py)\n",
    "ENV_NAME = \"ALE/BeamRider-v5\"\n",
    "\n",
    "env = gym.make(ENV_NAME, render_mode = \"rgb_array\").unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrappers and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipObservation: (210, 160, 3)\n",
      "ResizeObservation    : (84, 84, 3)\n",
      "GrayscaleObservation : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "ReshapeObservation   : (84, 84)\n",
      "FrameStackObservation: (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Implementation based on Pol Vierge's solution of the M3-3_Activity_1 and the Example_1 (REINFORCE baseline on CartPole)\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def make_env(env_name, render_mode=None):\n",
    "    env = gym.make(env_name, render_mode=render_mode)\n",
    "    print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n",
    "    env = MaxAndSkipObservation(env, skip = 2)\n",
    "    print(\"MaxAndSkipObservation: {}\".format(env.observation_space.shape))\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n",
    "    env = GrayscaleObservation(env, keep_dim=True)\n",
    "    print(\"GrayscaleObservation : {}\".format(env.observation_space.shape))\n",
    "    env = ImageToPyTorch(env)\n",
    "    print(\"ImageToPyTorch       : {}\".format(env.observation_space.shape))\n",
    "    env = ReshapeObservation(env, (84, 84))\n",
    "    print(\"ReshapeObservation   : {}\".format(env.observation_space.shape))\n",
    "    env = FrameStackObservation(env, stack_size=4)\n",
    "    print(\"FrameStackObservation: {}\".format(env.observation_space.shape))\n",
    "    env = ScaledFloatFrame(env)\n",
    "    print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n",
    "\n",
    "    return env\n",
    "\n",
    "env = make_env(ENV_NAME, render_mode = \"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer and DuelingDQN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPLAY BUFFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append(self.experience(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DQN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        # Fully connected layers for value and advantage streams\n",
    "        self.value_fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)  # Single value output for state value\n",
    "        )\n",
    "        self.advantage_fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)  # Advantage for each action\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        value = self.value_fc(conv_out)\n",
    "        advantage = self.advantage_fc(conv_out)\n",
    "        \n",
    "        # Combining value and advantage into Q-values\n",
    "        q_values = value + (advantage - advantage.mean())\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same implementation as in the training notebook, but this time with the added functionality to generate videos of the agent playing the game (wathc_agent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, replay_size=5000, batch_size=8, lr=1e-4, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.999, sync_target_frames=2000):\n",
    "        self.env = env\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.replay_buffer = ReplayBuffer(replay_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.sync_target_frames = sync_target_frames\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.policy_net = DuelingDQN(env.observation_space.shape, self.n_actions).to(self.device)\n",
    "        self.target_net = DuelingDQN(env.observation_space.shape, self.n_actions).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.frame_idx = 0\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\" \n",
    "        Selects an action using epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "        - state: Current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - action: Action to take in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state = torch.tensor([state], dtype=torch.float32).to(self.device)\n",
    "        q_values = self.policy_net(state)\n",
    "        return q_values.max(1)[1].item()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Samples a batch from the replay buffer and performs a single step of optimization.\n",
    "\n",
    "        Returns:\n",
    "        - loss: Loss value from the optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return 0.0\n",
    "\n",
    "        self.frame_idx += 1\n",
    "        if self.frame_idx % self.sync_target_frames == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_net(next_states).max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    # Visualization function for reward plot\n",
    "    def visualise(self, score, run_folder, show_avg=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - score: List of rewards per episode.\n",
    "        - run_folder: Folder to save the plot.\n",
    "        - show_avg: Whether to show the moving average of rewards.\n",
    "\n",
    "        Returns:\n",
    "        - None, it saves the plot in the run folder.\n",
    "        \"\"\"\n",
    "\n",
    "        score = np.array(score)\n",
    "\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.ylabel(\"Trajectory duration\", fontsize=12)\n",
    "        plt.xlabel(\"Training iterations\", fontsize=12)\n",
    "        plt.plot(score, color='gray', linewidth=1, label='Score')\n",
    "\n",
    "        if show_avg:\n",
    "            N = 100\n",
    "            avg_score = np.convolve(score, np.ones(N) / N, mode='valid')\n",
    "            plt.plot(avg_score, color='blue', linewidth=3, label='Moving Average (window=100)')\n",
    "\n",
    "        plt.scatter(np.arange(score.shape[0]), score, color='green', linewidth=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "\n",
    "        # Save the plot\n",
    "        rewards_plot_path = os.path.join(run_folder, \"test_reward_plot.png\")\n",
    "        plt.savefig(rewards_plot_path)\n",
    "        plt.close()  # Close the plot to avoid display in Jupyter/other environments\n",
    "        print(f\"Rewards plot saved at: {rewards_plot_path}\")\n",
    "\n",
    "    # Loss plot function\n",
    "    def plot_loss(self, losses, run_folder):\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.ylabel(\"Loss\", fontsize=12)\n",
    "        plt.xlabel(\"Training iterations\", fontsize=12)\n",
    "        plt.plot(losses, color='gray', linewidth=1, label='Loss')\n",
    "\n",
    "        # Save the plot\n",
    "        losses_plot_path = os.path.join(run_folder, \"loss_plot.png\")\n",
    "        plt.savefig(losses_plot_path)\n",
    "        plt.close()  # Close the plot to avoid display in Jupyter/other environments\n",
    "        print(f\"Loss plot saved at: {losses_plot_path}\")\n",
    "\n",
    "    # Watch agent function to evaluate the agent and save a GIF of the best episode.\n",
    "    def watch_agent(self, model_path=None, T=500, episodes=10, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - model_path: Path to the trained model weights (optional).\n",
    "        - T: Maximum steps per episode.\n",
    "        - episodes: Number of episodes to evaluate.\n",
    "        \"\"\"\n",
    "        if model_path:\n",
    "            self.policy_net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            self.policy_net.eval()\n",
    "\n",
    "        scores = []\n",
    "        episode_images = []\n",
    "\n",
    "        for episode in tqdm(range(episodes), desc=\"Evaluating episodes\"):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.array(state)  # Ensure state is in the right format\n",
    "            images = []\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            for t in range(T):\n",
    "                # Capture the rendered image for the GIF\n",
    "                img = self.env.render()\n",
    "                images.append(Image.fromarray(img))\n",
    "\n",
    "                # Select action using the policy network\n",
    "                state_tensor = torch.tensor([state], dtype=torch.float32).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.policy_net(state_tensor)\n",
    "                action = q_values.max(1)[1].item()\n",
    "\n",
    "                # Take the selected action\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            scores.append(total_reward)\n",
    "            episode_images.append(images)\n",
    "\n",
    "        # Save a GIF of the best episode\n",
    "        best_episode = np.argmax(scores)\n",
    "        best_images = episode_images[best_episode]\n",
    "        gif_path = \"best_episode.gif\"\n",
    "        best_images[0].save(\n",
    "            gif_path,\n",
    "            save_all=True,\n",
    "            append_images=best_images[1:],\n",
    "            duration=60,\n",
    "            loop=0\n",
    "        )\n",
    "        print(f\"Best episode GIF saved at: {gif_path}\")\n",
    "\n",
    "        # Plot scores\n",
    "        self.visualise(scores, run_folder=\".\", show_avg=False)\n",
    "\n",
    "        # Close the environment\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z9/pq761jhd4zv939bx11btv4m00000gn/T/ipykernel_6278/3813057591.py:126: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.policy_net.load_state_dict(torch.load(model_path, map_location=device))\n",
      "Evaluating episodes:   0%|          | 0/100 [00:00<?, ?it/s]/var/folders/z9/pq761jhd4zv939bx11btv4m00000gn/T/ipykernel_6278/3813057591.py:145: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  state_tensor = torch.tensor([state], dtype=torch.float32).to(self.device)\n",
      "Evaluating episodes: 100%|██████████| 100/100 [13:16<00:00,  7.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best episode GIF saved at: best_episode.gif\n",
      "Rewards plot saved at: ./test_reward_plot.png\n"
     ]
    }
   ],
   "source": [
    "# Create the agent\n",
    "agent = DQNAgent(env)\n",
    "\n",
    "# Watch the agent perform and save a GIF of the best episode\n",
    "agent.watch_agent(model_path=\"dqn_model.pth\", T=5000, episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paradigms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
