{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecVideoRecorder\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create Test Run Folder\n",
    "def create_test_folder(base_folder=\"test_ppo\"):\n",
    "    \"\"\"\n",
    "    Create a timestamped folder for each test run inside the `test` directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    test_run_folder = os.path.join(base_folder, f\"test_run_{timestamp}\")\n",
    "    os.makedirs(test_run_folder, exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_run_folder, \"videos\"), exist_ok=True)\n",
    "    return test_run_folder\n",
    "\n",
    "# Step 2: Save Reward Plot\n",
    "def save_plot(rewards, moving_avg, output_folder):\n",
    "    \"\"\"\n",
    "    Save a plot of rewards and their moving average.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(rewards, label=\"Score\", color=\"indigo\", alpha=0.6)\n",
    "    if moving_avg is not None:\n",
    "        plt.plot(moving_avg, label=\"Moving Average (window=10)\", color=\"blue\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.title(\"Reward Progression over 100 Episodes\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plot_path = os.path.join(output_folder, \"rewards_plot.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "# Step 3: Record Video\n",
    "def record_video(model, env, video_folder, video_length=1000):\n",
    "    \"\"\"\n",
    "    Record a gameplay video.\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    env = VecVideoRecorder(\n",
    "        env,\n",
    "        video_folder,\n",
    "        record_video_trigger=lambda x: x == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=\"test_performance\",\n",
    "    )\n",
    "    obs = env.reset()\n",
    "    video_rewards = []\n",
    "    done_count = 0\n",
    "    episode_reward = 0\n",
    "    for _ in range(video_length):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        #video_rewards.append(reward)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            done_count += 1\n",
    "            video_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "        if done_count == 5:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    return video_rewards\n",
    "\n",
    "# function to run n episodes\n",
    "def run_episodes(model, env, n_episodes=100):\n",
    "    \"\"\"\n",
    "    Run the model for n episodes and return the rewards.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        video_rewards = []\n",
    "        done_count = 0\n",
    "        episode_reward = 0\n",
    "        while done_count < 5:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            #video_rewards.append(reward)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                done_count += 1\n",
    "                video_rewards.append(episode_reward)\n",
    "                episode_reward = 0\n",
    "        rewards.append(video_rewards)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Step 4: Preprocess Atari Environment\n",
    "def create_env(env_name=\"BreakoutNoFrameskip-v4\", frame_stack=4):\n",
    "    def make_env():\n",
    "        gym.register_envs(ale_py)\n",
    "        env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        env = AtariWrapper(env, clip_reward=False)\n",
    "        return env\n",
    "\n",
    "    env = DummyVecEnv([make_env])\n",
    "    env = VecFrameStack(env, n_stack=frame_stack)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "# Set up test folder\n",
    "test_folder = create_test_folder()\n",
    "# Create environment\n",
    "env = create_env()\n",
    "# Load the saved PPO model\n",
    "model_path = \"models/ppo_breakout.zip\"\n",
    "model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video /Users/martinsssssss/Desktop/7th Semester/Paradigms of ML/Project/GIT/part2/test_ppo/test_run_20241217-002134/videos/test_performance-step-0-to-step-10000000.mp4.\n",
      "MoviePy - Writing video /Users/martinsssssss/Desktop/7th Semester/Paradigms of ML/Project/GIT/part2/test_ppo/test_run_20241217-002134/videos/test_performance-step-0-to-step-10000000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /Users/martinsssssss/Desktop/7th Semester/Paradigms of ML/Project/GIT/part2/test_ppo/test_run_20241217-002134/videos/test_performance-step-0-to-step-10000000.mp4\n",
      "Video rewards: 32.0\n",
      "Gameplay video saved in: test_ppo/test_run_20241217-002134/videos\n"
     ]
    }
   ],
   "source": [
    "video_folder = os.path.join(test_folder, \"videos\")\n",
    "\n",
    "# set the video length so it is the same as the number of steps in the episode\n",
    "rewards = record_video(model, env, video_folder, video_length= 10000000)\n",
    "save_plot(rewards, None, test_folder)\n",
    "\n",
    "# Save the rewards plot\n",
    "print(f\"Video rewards: {np.sum(rewards)}\")\n",
    "print(f\"Gameplay video saved in: {video_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 episodes: 22.68000030517578\n"
     ]
    }
   ],
   "source": [
    "rewards = run_episodes(model, env, n_episodes=100)\n",
    "ep_rewards = [np.sum(episode_rewards) for episode_rewards in rewards]\n",
    "print(f\"Average reward over 100 episodes: {np.mean(ep_rewards)}\")\n",
    "\n",
    "save_plot(ep_rewards, None, test_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paradigms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
